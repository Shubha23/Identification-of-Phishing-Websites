{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a09436c96ad9372631bc6da48ea6482c4ee3729"},"cell_type":"code","source":"import numpy as np  \nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Read the data\ndata = pd.read_csv('../input/Dataset.csv')\n\n# View first and last 5 observations\nprint(data.head())\nprint(data.tail())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Describe statistical information of data\nprint(data.describe())\n# Below stats show that 75 percentile of obseravtions belong to class 1 ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7175d247b7203e97252df9639e884b45d26eb956"},"cell_type":"code","source":"# Check column types\nprint(data.info())               \n\n# All comumns are int type, so no change is required","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc622c7b07162ac97eccff929cf718c9651d1bec"},"cell_type":"code","source":"# Plot distribution of classes using Histograms\nplt.figure(figsize =(8,8))\nplt.hist(data.Result)           \n\n# It shows that benign class have about 1000+ observations than malware","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a3dd52b04caa22ccc1c96ad98c78198a9d0bff8d"},"cell_type":"code","source":"# Look for missing values\nprint(data.isnull().sum())        \n\n# No missing values found, so no need to drop or replace any value","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dc959131654765e057490e4b518043c3016c0fdc"},"cell_type":"code","source":"# Generate correlation matrix\nprint(data.corr())\n\nimport seaborn as sns\nplt.figure(figsize =(8,8))\nsns.heatmap(data.corr())    # Generate heatmap (though very less clarity due to large no. of ftrs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f844ee7d55a01fc66822732e732f99233bb09c20"},"cell_type":"code","source":"print(data.corr()['Result'].sort_values())      # Print correlation with target variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"608a7dab5dd299514b79b287c8e32e1ae5f1861d"},"cell_type":"code","source":"# Remove features having correlation coeff. between +/- 0.03\ndata.drop(['Favicon','Iframe','Redirect',\n                'popUpWidnow','RightClick','Submitting_to_email'],axis=1,inplace=True)\nprint(len(data.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"9a4a48e23077eb8edac2cfedc79c7b45ac65ad2e"},"cell_type":"code","source":"# Prepare data for models\ny = data['Result'].values\nX = data.drop(['Result'], axis = 1)\n\nfrom sklearn.metrics import accuracy_score,roc_curve,auc, confusion_matrix\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\n# Split the data as training and testing data - 70% train size, 30% test size\nX_train, X_test, y_train, y_test  = train_test_split(X, y, test_size = 0.3, random_state = None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eea72bc6acffa1503567fc46f0f6d613dfefadfd"},"cell_type":"code","source":"#1 Classification using Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc = rfc.fit(X_train,y_train)\nprediction = rfc.predict(X_test)\nprint(\"Accuracy with RF classifier:\",accuracy_score(y_test, prediction)) \nfpr,tpr,thresh = roc_curve(y_test,prediction)      \nroc_auc = accuracy_score(y_test,prediction)         # Calculate ROC AUC\n\n# Plot ROC curve for Random Forest\nplt.plot(fpr,tpr,'g',label = 'Random Forest')\nplt.legend(\"Random Forest\", loc='lower right')\nplt.legend(loc='lower right')\nprint(\"Conf matrix RF classifier:\",confusion_matrix(y_test,prediction))  #  Generate confusion matrix\n\n#2 Classification using logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg = logreg.fit(X_train,y_train)\nprediction = logreg.predict(X_test)\nprint(\"Accuracy with Log Reg:\", accuracy_score(y_test, prediction))\nprint (\"Conf matrix Log Reg:\",confusion_matrix(y_test,prediction))\nfpr,tpr,thresh = roc_curve(y_test,prediction)\nroc_auc = accuracy_score(y_test,prediction)\n\n# Plot ROC curve for Logistic Regression\nplt.plot(fpr,tpr,'orange',label = 'Logistic Regression')\nplt.legend(\"Logistic Regression\", loc='lower right')\nplt.xlabel(\"False positive rate\")\nplt.ylabel(\"True positive rate\")\nplt.legend(loc='lower right')\n\n#3 Classification using SVM\nfrom sklearn.svm import SVC\nsvc_l = SVC(kernel = \"linear\", C = 0.025)\nsvc_l = svc_l.fit(X_train,y_train)\nprediction = svc_l.predict(X_test)\nprint(\"Accuracy with SVM-Linear:\",accuracy_score(y_test, prediction))\nfpr,tpr,thresh = roc_curve(y_test,prediction)\nroc_auc = accuracy_score(y_test,prediction)\n\n# Plot ROC curve for SVM-linear\nplt.plot(fpr,tpr,'b',label = 'SVM')\nplt.legend(\"SVM\", loc ='lower right')\nplt.legend(loc ='lower right')\nprint(\"Conf matrix SVM-linear:\",confusion_matrix(y_test,prediction))\n\nplt.show()\n\n'''\n# -------- Apply Recursive Feature Elimination(RFE) and use reduced feature set for prediction ------------------------\n# Recursive Feature Elimination(RFE) is a technique that takes entire feature set as input and removes features one at \n# a time up to a specified number or until a stopping criteria is met.\n'''\nfrom sklearn.feature_selection import RFE\nrfe = RFE(rfc,27)                              \nrfe = rfe.fit(X_train, y_train)               # Train RF classifier with only 27 features now\npred = rfe.predict(X_test)\n\n# Test accuracy on reduced data\nprint(\"Accuracy by RFClassifier after RFE is applied:\", accuracy_score(y_test,pred))\n\nrfe = RFE(svc_l,27)\nrfe = rfe.fit(X_train, y_train)               # Train SVM with only 27 features now\npred = rfe.predict(X_test)\nprint(\"Accuracy by SVM-Linear after RFE is applied:\", accuracy_score(y_test,pred))\n\nrfe = RFE(logreg,27)\nrfe = rfe.fit(X_train, y_train)              # Train Logistic-Reg with only 27 features now\npred = rfe.predict(X_test)\nprint(\"Accuracy by Logistic Regression after RFE is applied:\", accuracy_score(y_test,pred))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}